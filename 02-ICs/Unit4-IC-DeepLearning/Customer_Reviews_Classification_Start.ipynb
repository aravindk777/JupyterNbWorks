{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321b4b0cfbba83fa",
   "metadata": {},
   "source": [
    "Unit 4 Incremental\n",
    "\n",
    "Part 3 - Customer Reviews starter code\n",
    "\n",
    "Matt Lucas - 2509"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a335ca2927eec",
   "metadata": {},
   "source": [
    "In this project, only concern yourself with reading the review title, the review text, and then have the model predict whether or not that was a good or a bad review.\n",
    "\n",
    "This is a very hard project! The dataset is imbalanced, and your model would love to predict that all the reviews are positive. The NLP upcoming NLP and LLM modules and the provided capstone projects will give more practice and tools to improve sentiment analysis. Consider it a win if you can get your model to not be greedy (guess that all the reviews are positive/negative).\n",
    "\n",
    "My model's confusion matrix was as follows:\n",
    "\n",
    "[40  39\n",
    "\n",
    "304 1526]\n",
    "\n",
    "Results will vary, but what you don't want is all the numbers in the left column or all the numbers in the right column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00239450be54a",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783421dbd82376bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:52.612733Z",
     "start_time": "2026-01-08T11:36:47.184362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data file: ./GrammarandProductReviews.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewlucas/development/mlprojects/Unit4_Incremental_MLTA/.venv/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "DATA_PATH = \"./GrammarandProductReviews.xlsx\"\n",
    "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"\n",
    "print(\"Using data file:\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4e65bea6355c3",
   "metadata": {},
   "source": [
    "To be able to read the excel file, you may need to use this in the terminal: pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b401869f86373",
   "metadata": {},
   "source": [
    "load the dataset and view the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d3dd7067b17c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:53.873865Z",
     "start_time": "2026-01-08T11:36:52.620969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>dateAdded</th>\n",
       "      <th>dateUpdated</th>\n",
       "      <th>ean</th>\n",
       "      <th>keys</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>manufacturerNumber</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.id</th>\n",
       "      <th>reviews.numHelpful</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "      <th>upc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AV13O1A8GV-KLJ3akUyj</td>\n",
       "      <td>Universal Music</td>\n",
       "      <td>Movies, Music &amp; Books,Music,R&amp;b,Movies &amp; TV,Mo...</td>\n",
       "      <td>2017-07-25T00:52:42Z</td>\n",
       "      <td>2018-02-05T08:36:58Z</td>\n",
       "      <td>6.025370e+11</td>\n",
       "      <td>602537205981,universalmusic/14331328,universal...</td>\n",
       "      <td>Universal Music Group / Cash Money</td>\n",
       "      <td>14331328</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>https://redsky.target.com/groot-domain-api/v1/...</td>\n",
       "      <td>i love this album. it's very good. more to the...</td>\n",
       "      <td>Just Awesome</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joshua</td>\n",
       "      <td>6.025370e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>2017-07-25T05:16:03Z</td>\n",
       "      <td>2018-02-05T11:27:45Z</td>\n",
       "      <td>7.341600e+10</td>\n",
       "      <td>lundbergorganiccinnamontoastricecakes/b000fvzw...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>574764</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>...</td>\n",
       "      <td>100209113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.walmart.com/reviews/product/29775278</td>\n",
       "      <td>Good flavor. This review was collected as part...</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorothy W</td>\n",
       "      <td>7.341600e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>2017-07-25T05:16:03Z</td>\n",
       "      <td>2018-02-05T11:27:45Z</td>\n",
       "      <td>7.341600e+10</td>\n",
       "      <td>lundbergorganiccinnamontoastricecakes/b000fvzw...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>574764</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>...</td>\n",
       "      <td>100209113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.walmart.com/reviews/product/29775278</td>\n",
       "      <td>Good flavor.</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorothy W</td>\n",
       "      <td>7.341600e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>2017-07-25T16:26:19Z</td>\n",
       "      <td>2018-02-05T11:25:51Z</td>\n",
       "      <td>6.798193e+10</td>\n",
       "      <td>kylovesensualitypleasuregel/b00u2whx8s,0679819...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>67981934427</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>...</td>\n",
       "      <td>113026909.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.walmart.com/reviews/product/43383370</td>\n",
       "      <td>I read through the reviews on here before look...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>6.798193e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>2017-07-25T16:26:19Z</td>\n",
       "      <td>2018-02-05T11:25:51Z</td>\n",
       "      <td>6.798193e+10</td>\n",
       "      <td>kylovesensualitypleasuregel/b00u2whx8s,0679819...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>67981934427</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>...</td>\n",
       "      <td>171267657.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.walmart.com/reviews/product/43383370</td>\n",
       "      <td>My husband bought this gel for us. The gel cau...</td>\n",
       "      <td>Irritation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Walker557</td>\n",
       "      <td>6.798193e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id            brand  \\\n",
       "0  AV13O1A8GV-KLJ3akUyj  Universal Music   \n",
       "1  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "2  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "3  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "4  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "\n",
       "                                          categories             dateAdded  \\\n",
       "0  Movies, Music & Books,Music,R&b,Movies & TV,Mo...  2017-07-25T00:52:42Z   \n",
       "1  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...  2017-07-25T05:16:03Z   \n",
       "2  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...  2017-07-25T05:16:03Z   \n",
       "3  Personal Care,Medicine Cabinet,Lubricant/Sperm...  2017-07-25T16:26:19Z   \n",
       "4  Personal Care,Medicine Cabinet,Lubricant/Sperm...  2017-07-25T16:26:19Z   \n",
       "\n",
       "            dateUpdated           ean  \\\n",
       "0  2018-02-05T08:36:58Z  6.025370e+11   \n",
       "1  2018-02-05T11:27:45Z  7.341600e+10   \n",
       "2  2018-02-05T11:27:45Z  7.341600e+10   \n",
       "3  2018-02-05T11:25:51Z  6.798193e+10   \n",
       "4  2018-02-05T11:25:51Z  6.798193e+10   \n",
       "\n",
       "                                                keys  \\\n",
       "0  602537205981,universalmusic/14331328,universal...   \n",
       "1  lundbergorganiccinnamontoastricecakes/b000fvzw...   \n",
       "2  lundbergorganiccinnamontoastricecakes/b000fvzw...   \n",
       "3  kylovesensualitypleasuregel/b00u2whx8s,0679819...   \n",
       "4  kylovesensualitypleasuregel/b00u2whx8s,0679819...   \n",
       "\n",
       "                         manufacturer manufacturerNumber  \\\n",
       "0  Universal Music Group / Cash Money           14331328   \n",
       "1                            Lundberg             574764   \n",
       "2                            Lundberg             574764   \n",
       "3                                 K-Y        67981934427   \n",
       "4                                 K-Y        67981934427   \n",
       "\n",
       "                                         name  ...   reviews.id  \\\n",
       "0   Pink Friday: Roman Reloaded Re-Up (w/dvd)  ...          NaN   \n",
       "1  Lundberg Organic Cinnamon Toast Rice Cakes  ...  100209113.0   \n",
       "2  Lundberg Organic Cinnamon Toast Rice Cakes  ...  100209113.0   \n",
       "3            K-Y Love Sensuality Pleasure Gel  ...  113026909.0   \n",
       "4            K-Y Love Sensuality Pleasure Gel  ...  171267657.0   \n",
       "\n",
       "  reviews.numHelpful reviews.rating  \\\n",
       "0                0.0              5   \n",
       "1                NaN              5   \n",
       "2                NaN              5   \n",
       "3                NaN              1   \n",
       "4                NaN              1   \n",
       "\n",
       "                                  reviews.sourceURLs  \\\n",
       "0  https://redsky.target.com/groot-domain-api/v1/...   \n",
       "1   https://www.walmart.com/reviews/product/29775278   \n",
       "2   https://www.walmart.com/reviews/product/29775278   \n",
       "3   https://www.walmart.com/reviews/product/43383370   \n",
       "4   https://www.walmart.com/reviews/product/43383370   \n",
       "\n",
       "                                        reviews.text  reviews.title  \\\n",
       "0  i love this album. it's very good. more to the...   Just Awesome   \n",
       "1  Good flavor. This review was collected as part...           Good   \n",
       "2                                       Good flavor.           Good   \n",
       "3  I read through the reviews on here before look...   Disappointed   \n",
       "4  My husband bought this gel for us. The gel cau...     Irritation   \n",
       "\n",
       "   reviews.userCity  reviews.userProvince reviews.username           upc  \n",
       "0       Los Angeles                   NaN           Joshua  6.025370e+11  \n",
       "1               NaN                   NaN        Dorothy W  7.341600e+10  \n",
       "2               NaN                   NaN        Dorothy W  7.341600e+10  \n",
       "3               NaN                   NaN          Rebecca  6.798193e+10  \n",
       "4               NaN                   NaN        Walker557  6.798193e+10  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78af3ee3f07f872",
   "metadata": {},
   "source": [
    "I'm only going to use reviews.title, reviews.text, and reviews.rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d911a2a90b92feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:53.981641Z",
     "start_time": "2026-01-08T11:36:53.974837Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df[[\"reviews.title\", \"reviews.text\", \"reviews.rating\"]].copy()\n",
    "\n",
    "data[\"text\"] = (\n",
    "    data[\"reviews.title\"].astype(str) + \" \" +\n",
    "    data[\"reviews.text\"].astype(str)\n",
    ")\n",
    "\n",
    "data[\"rating\"] = pd.to_numeric(\n",
    "    data[\"reviews.rating\"],\n",
    "    errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d181fdbaf99707",
   "metadata": {},
   "source": [
    "Remove missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3e8c01ff64616d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:54.003434Z",
     "start_time": "2026-01-08T11:36:53.996403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just Awesome i love this album. it's very good...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good Good flavor. This review was collected as...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good Good flavor.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disappointed I read through the reviews on her...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Irritation My husband bought this gel for us. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  rating\n",
       "0  Just Awesome i love this album. it's very good...       5\n",
       "1  Good Good flavor. This review was collected as...       5\n",
       "2                                  Good Good flavor.       5\n",
       "3  Disappointed I read through the reviews on her...       1\n",
       "4  Irritation My husband bought this gel for us. ...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"text\", \"rating\"]].copy()\n",
    "\n",
    "data = data.dropna(subset=[\"text\", \"rating\"]).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c6ca3384e72a1",
   "metadata": {},
   "source": [
    "Create a binary positive/negative for reviews. out of 5 stars, 2 and 1 are negative, 4 and 5 are positive. 3's are turned into n/a values and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca6e5d76ab83867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:54.039521Z",
     "start_time": "2026-01-08T11:36:54.033850Z"
    }
   },
   "outputs": [],
   "source": [
    "def rating_to_label(r: float) -> float:\n",
    "    if r >= 4:\n",
    "        return 1\n",
    "    if r <= 2:\n",
    "        return 0\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "data[\"label\"] = data[\"rating\"].apply(rating_to_label)\n",
    "data = data.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "data[\"label\"] = data[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8454577d0668d8",
   "metadata": {},
   "source": [
    "some light data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc121ad2d5fef3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:54.156457Z",
     "start_time": "2026-01-08T11:36:54.077131Z"
    }
   },
   "outputs": [],
   "source": [
    "url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "html_re = re.compile(r\"<.*?>\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = url_re.sub(\" \", s)\n",
    "    s = html_re.sub(\" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\.,!?\\'\\\"\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "data[\"text_clean\"] = data[\"text\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33457a0141a25c9",
   "metadata": {},
   "source": [
    "Remove reviews that are too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849217ac2e702465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:54.169295Z",
     "start_time": "2026-01-08T11:36:54.161853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 reviews with 3 or fewer characters; remaining: 9548\n"
     ]
    }
   ],
   "source": [
    "MIN_REVIEW_LENGTH = 3\n",
    "before = len(data)\n",
    "data = data[data[\"text_clean\"].str.len() > MIN_REVIEW_LENGTH].reset_index(drop=True)\n",
    "print(f\"Removed {before - len(data)} reviews with {MIN_REVIEW_LENGTH} or fewer characters; remaining: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31827c448ea7676b",
   "metadata": {},
   "source": [
    "Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f105ddb8b89e31df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:36:54.188592Z",
     "start_time": "2026-01-08T11:36:54.178335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 7638 Test size: 1910\n",
      "Training class distribution:\n",
      "0     315\n",
      "1    7323\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data[\"text_clean\"].values\n",
    "y = data[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
    "print(\"Training class distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a9a62ac42fb6a",
   "metadata": {},
   "source": [
    "As you can see, the training set is overwhelmingly positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b699db2",
   "metadata": {},
   "source": [
    "## Tokenize and Pad Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "# TODO: Set VOCAB_SIZE to 20000 and MAX_LEN to 200\n",
    "VOCAB_SIZE = ___\n",
    "MAX_LEN = ___\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "# TODO: Convert texts to sequences for both training and test sets\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# TODO: Pad sequences using pad_sequences\n",
    "# Parameters: maxlen=MAX_LEN, padding=\"post\", truncating=\"post\"\n",
    "X_train_pad = pad_sequences(___, maxlen=___, padding=___, truncating=___)\n",
    "X_test_pad = pad_sequences(___, maxlen=___, padding=___, truncating=___)\n",
    "\n",
    "print(\"Padded train shape:\", X_train_pad.shape)\n",
    "print(\"Padded test shape:\", X_test_pad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af651834",
   "metadata": {},
   "source": [
    "## Apply Undersampling to Balance Classes\n",
    "\n",
    "SMOTE is not a great way to balance this dataset because it is not numerical, there's no real way to synthetically create text reviews between the clusters.\n",
    "\n",
    "**Resources for learning about undersampling:**\n",
    "- [Handling Imbalanced Data for Classification - GeeksforGeeks](https://www.geeksforgeeks.org/handling-imbalanced-data-for-classification/) - Comprehensive guide on undersampling, oversampling, and other techniques for handling class imbalance\n",
    "- [imbalanced-learn Documentation - RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html) - Official API documentation for RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bd728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply undersampling to balance classes (1:3 ratio to keep more data)\n",
    "print(\"\\nApplying Random Undersampling with 1:3 ratio...\")\n",
    "print(f\"Before undersampling: {X_train_pad.shape[0]} samples\")\n",
    "print(\"Class distribution before:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "\n",
    "# Count minority class samples\n",
    "# TODO: Count how many samples belong to the minority class (label 0)\n",
    "minority_count = np.sum(___ == ___)\n",
    "\n",
    "# Calculate target for majority class (keep 3x the minority class)\n",
    "# TODO: Set majority_target to be 3 times the minority_count\n",
    "majority_target = ___ * ___\n",
    "\n",
    "print(f\"\\nMinority class (negative): {minority_count} samples\")\n",
    "print(f\"Target majority class (positive): {majority_target} samples\")\n",
    "print(f\"(Using 1:3 ratio instead of 1:1 to keep more training data)\")\n",
    "\n",
    "# Use undersampling with custom ratio\n",
    "# TODO: Create a RandomUnderSampler with:\n",
    "# - random_state=SEED\n",
    "# - sampling_strategy={0: minority_count, 1: majority_target}\n",
    "undersampler = RandomUnderSampler(\n",
    "    random_state=___,\n",
    "    sampling_strategy={0: ___, 1: ___}\n",
    ")\n",
    "\n",
    "# TODO: Apply fit_resample to X_train_pad and y_train\n",
    "X_train_balanced, y_train_balanced = undersampler.___(___, ___)\n",
    "\n",
    "print(f\"\\nAfter undersampling: {X_train_balanced.shape[0]} samples\")\n",
    "print(\"Class distribution after:\")\n",
    "print(pd.Series(y_train_balanced).value_counts().sort_index())\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build the model",
   "id": "5de0ed8835a2a8fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build CNN-LSTM hybrid model\n",
    "# TODO: Set EMBED_DIM to 100 (embedding dimension for word vectors)\n",
    "EMBED_DIM = ___\n",
    "\n",
    "# TODO: Build a Sequential model with the following layers:\n",
    "# 1. Embedding layer: input_dim=VOCAB_SIZE, output_dim=EMBED_DIM\n",
    "# 2. Conv1D layer: filters=64, kernel_size=5, activation='relu'\n",
    "# 3. MaxPooling1D layer: pool_size=2\n",
    "# 4. Dropout layer: rate=0.3\n",
    "# 5. LSTM layer: units=32, return_sequences=False\n",
    "# 6. Dropout layer: rate=0.3\n",
    "# 7. Dense layer: units=32, activation='relu'\n",
    "# 8. Dropout layer: rate=0.3\n",
    "# 9. Dense output layer: units=1, activation='sigmoid'\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=___, output_dim=___),\n",
    "    layers.Conv1D(filters=___, kernel_size=___, activation=___),\n",
    "    layers.MaxPooling1D(pool_size=___),\n",
    "    layers.Dropout(___),\n",
    "    layers.LSTM(___, return_sequences=___),\n",
    "    layers.Dropout(___),\n",
    "    layers.Dense(___, activation=___),\n",
    "    layers.Dropout(___),\n",
    "    layers.Dense(___, activation=___),\n",
    "])\n",
    "\n",
    "# TODO: Compile the model with:\n",
    "# - optimizer: tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "# - loss: 'binary_crossentropy'\n",
    "# - metrics: ['accuracy']\n",
    "model.compile(\n",
    "    optimizer=___,\n",
    "    loss=___,\n",
    "    metrics=[___]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "769152b3f4514278"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the Model",
   "id": "b0af52dd5b8773c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=5,  # Increased patience\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Add ModelCheckpoint to save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Split balanced data for validation\n",
    "from sklearn.model_selection import train_test_split as sk_split\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = sk_split(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_train_balanced\n",
    ")\n",
    "\n",
    "print(f\"Training on balanced data: {len(X_train_final)} samples\")\n",
    "print(f\"Validation: {len(X_val_final)} samples\")\n",
    "print(f\"Training class distribution:\")\n",
    "print(pd.Series(y_train_final).value_counts().sort_index())\n",
    "\n",
    "# Add moderate class weights based on the balanced training data\n",
    "# Since we're using 1:3 ratio, adjust weights accordingly\n",
    "classes = np.array([0, 1])\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_balanced)\n",
    "# Moderate weighting - cap at 2.0 to prevent over-weighting (reduced from 2.5)\n",
    "class_weight = {0: min(float(weights[0]), 2.0), 1: 1.0}\n",
    "print(f\"\\nUsing class weights: {class_weight}\")\n",
    "print(\"(Moderate weight on negative class - capped at 2.0 to prevent extreme bias)\")\n",
    "print(\"\\nNote: Model architecture simplified to help learn better class distinctions\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    validation_data=(X_val_final, y_val_final),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,  # Add class weights for extra emphasis\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n"
   ],
   "id": "368a75ae9f1b9255"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Evaluate the Model\n",
    "\n",
    "NOTE: check the confusion matrix to ensure that all the values are not either all positive or all negative. If they are all on one side, your model is getting greedy. You'll want to tinker with the model to get it to try to learn both positive and negative sentiments."
   ],
   "id": "84c7de0f13458b05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 9) Evaluate\n",
    "\n",
    "# First, check training history to ensure model learned\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING HISTORY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if 'loss' in history.history:\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"Final Training Loss: {final_train_loss:.4f}, Accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss:.4f}, Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "    # Check if model improved\n",
    "    if len(history.history['loss']) > 1:\n",
    "        initial_loss = history.history['loss'][0]\n",
    "        loss_improvement = initial_loss - final_train_loss\n",
    "        print(f\"Loss improvement: {loss_improvement:.4f}\")\n",
    "        if loss_improvement < 0.01:\n",
    "            print(\"âš ï¸  WARNING: Model may not be learning (minimal loss improvement)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate loss + accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predict probabilities\n",
    "y_prob = model.predict(X_test_pad, verbose=0).ravel()\n",
    "\n",
    "# ---- Sanity check on probabilities FIRST ----\n",
    "print(\"\\nProbability Statistics:\")\n",
    "print(f\"Mean: {np.mean(y_prob):.4f}\")\n",
    "print(f\"Min: {np.min(y_prob):.4f}, Max: {np.max(y_prob):.4f}\")\n",
    "print(f\"Std: {np.std(y_prob):.4f}\")\n",
    "\n",
    "# Check if probabilities are too clustered (model not learning)\n",
    "if np.std(y_prob) < 0.01:\n",
    "    print(\"âš ï¸  WARNING: Probabilities are too clustered - model may not be learning!\")\n",
    "\n",
    "# ---- Find optimal threshold using multiple methods ----\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Method 1: Youden's J statistic (maximizes TPR - FPR)\n",
    "youden_j = tpr - fpr\n",
    "optimal_idx_youden = np.argmax(youden_j)\n",
    "optimal_threshold_youden = thresholds[optimal_idx_youden]\n",
    "\n",
    "# Method 2: F1-score optimization (better for imbalanced data)\n",
    "f1_scores = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_temp = (y_prob >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_temp, zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "optimal_idx_f1 = np.argmax(f1_scores)\n",
    "optimal_threshold_f1 = thresholds[optimal_idx_f1]\n",
    "max_f1 = f1_scores[optimal_idx_f1]\n",
    "\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
    "if roc_auc < 0.5:\n",
    "    print(\"âš ï¸  WARNING: ROC AUC < 0.5 means model is worse than random!\")\n",
    "elif roc_auc < 0.6:\n",
    "    print(\"âš ï¸  WARNING: ROC AUC < 0.6 means model performance is poor\")\n",
    "\n",
    "print(f\"\\nThreshold Selection:\")\n",
    "print(f\"  Youden's J (max TPR-FPR): {optimal_threshold_youden:.4f}\")\n",
    "print(f\"  F1-score optimal: {optimal_threshold_f1:.4f} (F1 = {max_f1:.4f})\")\n",
    "\n",
    "# Compare both thresholds\n",
    "y_pred_youden = (y_prob >= optimal_threshold_youden).astype(int)\n",
    "y_pred_f1 = (y_prob >= optimal_threshold_f1).astype(int)\n",
    "\n",
    "f1_youden = f1_score(y_test, y_pred_youden, zero_division=0)\n",
    "recall_youden = recall_score(y_test, y_pred_youden, zero_division=0)\n",
    "precision_youden = precision_score(y_test, y_pred_youden, zero_division=0)\n",
    "\n",
    "f1_f1 = f1_score(y_test, y_pred_f1, zero_division=0)\n",
    "recall_f1 = recall_score(y_test, y_pred_f1, zero_division=0)\n",
    "precision_f1 = precision_score(y_test, y_pred_f1, zero_division=0)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Youden's threshold: Precision={precision_youden:.3f}, Recall={recall_youden:.3f}, F1={f1_youden:.3f}\")\n",
    "print(f\"  F1-optimal threshold: Precision={precision_f1:.3f}, Recall={recall_f1:.3f}, F1={f1_f1:.3f}\")\n",
    "\n",
    "# Choose threshold intelligently - avoid extreme values\n",
    "# If F1-optimal is too extreme (< 0.4 or > 0.7), use Youden's J instead\n",
    "# Also check if F1-optimal causes all predictions to be one class\n",
    "THRESHOLD_MIN = 0.4  # Increased from 0.3 to be more conservative\n",
    "THRESHOLD_MAX = 0.7\n",
    "\n",
    "# Check if F1-optimal threshold causes all predictions to be one class\n",
    "all_positive_f1 = np.all(y_pred_f1 == 1)\n",
    "all_negative_f1 = np.all(y_pred_f1 == 0)\n",
    "\n",
    "if optimal_threshold_f1 < THRESHOLD_MIN or optimal_threshold_f1 > THRESHOLD_MAX:\n",
    "    print(f\"\\nâš ï¸  F1-optimal threshold ({optimal_threshold_f1:.4f}) is too extreme!\")\n",
    "    print(f\"   Using Youden's J threshold ({optimal_threshold_youden:.4f}) instead for better balance.\")\n",
    "    optimal_threshold = optimal_threshold_youden\n",
    "    y_pred = y_pred_youden\n",
    "elif all_positive_f1 or all_negative_f1:\n",
    "    print(f\"\\nâš ï¸  F1-optimal threshold ({optimal_threshold_f1:.4f}) causes all predictions to be {'positive' if all_positive_f1 else 'negative'}!\")\n",
    "    print(f\"   Using Youden's J threshold ({optimal_threshold_youden:.4f}) instead for better balance.\")\n",
    "    optimal_threshold = optimal_threshold_youden\n",
    "    y_pred = y_pred_youden\n",
    "else:\n",
    "    print(f\"\\nâœ… Using F1-optimal threshold: {optimal_threshold_f1:.4f}\")\n",
    "    optimal_threshold = optimal_threshold_f1\n",
    "    y_pred = y_pred_f1\n",
    "\n",
    "# ---- Confusion Matrix + Report ----\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'':20s} {'Predicted: Negative':25s} {'Predicted: Positive':25s}\")\n",
    "print(f\"{'Actual: Negative':20s} {f'TN={tn}':25s} {f'FP={fp}':25s}\")\n",
    "print(f\"{'Actual: Positive':20s} {f'FN={fn}':25s} {f'TP={tp}':25s}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“Š KEY METRICS:\")\n",
    "print(f\"  Precision: {precision:.4f} (of predicted positive, how many are actually positive)\")\n",
    "print(f\"  Recall (Sensitivity): {recall:.4f} (of actual positive, how many did we catch)\")\n",
    "print(f\"  Specificity: {specificity:.4f} (of actual negative, how many did we correctly identify)\")\n",
    "print(f\"  F1-Score: {f1:.4f} (harmonic mean of precision and recall)\")\n",
    "print(f\"  False Negatives: {fn} (missed positive reviews - we want to minimize this)\")\n",
    "print(f\"  False Positives: {fp} (incorrectly flagged as positive)\")\n",
    "\n",
    "print(\n",
    "    \"\\nClassification Report:\\n\",\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\"negative\", \"positive\"],\n",
    "        zero_division=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---- Confusion Matrix Visualization ----\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "im = ax1.imshow(cm, cmap='Blues')\n",
    "ax1.figure.colorbar(im, ax=ax1)\n",
    "ax1.set_title(\"Confusion Matrix\", fontsize=14, fontweight='bold')\n",
    "tick_marks = np.arange(2)\n",
    "ax1.set_xticks(tick_marks)\n",
    "ax1.set_yticks(tick_marks)\n",
    "ax1.set_xticklabels([\"negative\", \"positive\"])\n",
    "ax1.set_yticklabels([\"negative\", \"positive\"])\n",
    "ax1.set_xlabel(\"Predicted Label\")\n",
    "ax1.set_ylabel(\"True Label\")\n",
    "\n",
    "# Annotate each cell\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax1.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontweight='bold', fontsize=12)\n",
    "\n",
    "# ROC Curve with both threshold markers\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "# Mark Youden's optimal\n",
    "ax2.plot(fpr[optimal_idx_youden], tpr[optimal_idx_youden], 'go', markersize=8,\n",
    "         label=f\"Youden's J = {optimal_threshold_youden:.4f}\")\n",
    "# Mark F1-optimal\n",
    "ax2.plot(fpr[optimal_idx_f1], tpr[optimal_idx_f1], 'ro', markersize=10,\n",
    "         label=f'F1-Optimal = {optimal_threshold:.4f} (used)')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4e040f43dbfd93d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the Training History",
   "id": "af90db3bd167e8a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 10) (Optional) Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history.get(\"accuracy\", []), label=\"train_accuracy\")\n",
    "plt.plot(history.history.get(\"val_accuracy\", []), label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n",
    "plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()\n"
   ],
   "id": "53cf39833f8186da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Questions to answer:\n",
    "\n",
    "Was the model greedy? What did you try to get it to check for both values?\n",
    "\n",
    "Compare this binary (positive or negative) To the NLP we just learned. "
   ],
   "id": "d87b91c6e13e5168"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
