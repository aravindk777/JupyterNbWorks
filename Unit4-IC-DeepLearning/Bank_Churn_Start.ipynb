{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Churn Prediction\n",
    "\n",
    "Matthew Lucas  \n",
    "Unit 4 Incremental Capstone  \n",
    "Class 2509 TA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean data\n",
    "bankData = pd.read_csv(\"Churn_Modeling.csv\")\n",
    "\n",
    "# Remove unnecessary features\n",
    "# TODO: Create a list of columns to drop (RowNumber, CustomerId, Surname)\n",
    "columns_to_drop = [___, ___, ___]\n",
    "bankData_cleaned = bankData.drop(columns=columns_to_drop)\n",
    "\n",
    "# Encode categorical variables\n",
    "# TODO: Map Gender: \"Female\" -> 0, \"Male\" -> 1\n",
    "bankData_cleaned[\"Gender\"] = bankData_cleaned[\"Gender\"].map({___: ___, ___: ___})\n",
    "\n",
    "# TODO: Map Geography: \"France\" -> 0, \"Germany\" -> 1, \"Spain\" -> 2\n",
    "bankData_cleaned[\"Geography\"] = bankData_cleaned[\"Geography\"].map({\n",
    "    ___: ___,\n",
    "    ___: ___,\n",
    "    ___: ___\n",
    "})\n",
    "\n",
    "# Define column types\n",
    "# TODO: Fill in the categorical and numerical column lists\n",
    "cat_cols = [___, ___, ___, ___, ___]\n",
    "num_cols = [___, ___, ___, ___, ___]\n",
    "\n",
    "print(f\"Dataset shape: {bankData_cleaned.shape}\")\n",
    "print(f\"Missing values: {bankData_cleaned.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split and Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (must be done before scaling)\n",
    "# TODO: Separate features (X) and target (y). Target column is \"Exited\"\n",
    "X = bankData_cleaned.drop(columns=[___])\n",
    "y = bankData_cleaned[___]\n",
    "\n",
    "# TODO: Split the data using train_test_split\n",
    "# Use test_size=0.2, random_state=42, and stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ___, ___, \n",
    "    test_size=___, \n",
    "    random_state=___, \n",
    "    stratify=___\n",
    ")\n",
    "\n",
    "# Scale numerical features (fit on training, transform both)\n",
    "# TODO: Create a StandardScaler and fit it on X_train[num_cols], then transform both X_train and X_test\n",
    "scaler = StandardScaler()\n",
    "X_train[num_cols] = scaler.___(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.___(X_test[num_cols])\n",
    "\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(f\"Class 0 (No Churn): {Counter(y_train)[0]}\")\n",
    "print(f\"Class 1 (Churn): {Counter(y_train)[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SMOTE and Calculate Class Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance training data\n",
    "# TODO: Create a SMOTE object with random_state=42, then fit_resample on X_train and y_train\n",
    "smote = SMOTE(random_state=___)\n",
    "X_train_balanced, y_train_balanced = smote.___(___, ___)\n",
    "\n",
    "# Calculate class weights\n",
    "# TODO: Use compute_class_weight with 'balanced' mode to calculate weights for y_train\n",
    "class_weights = compute_class_weight(___, classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(f\"Class 0 (No Churn): {Counter(y_train_balanced)[0]}\")\n",
    "print(f\"Class 1 (Churn): {Counter(y_train_balanced)[1]}\")\n",
    "print(f\"\\nTraining samples: {len(X_train)} -> {len(X_train_balanced)}\")\n",
    "print(f\"Class weights: {class_weight_dict}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a Sequential model with:\n",
    "# - Input layer with shape matching X_train_balanced.shape[1]\n",
    "# - Dense layer with 32 units and 'relu' activation\n",
    "# - Dense layer with 16 units and 'relu' activation  \n",
    "# - Dense output layer with 1 unit and 'sigmoid' activation\n",
    "model = Sequential([\n",
    "    Input(shape=(___,)),\n",
    "    Dense(___, activation=___),\n",
    "    Dense(___, activation=___),\n",
    "    Dense(___, activation=___)\n",
    "])\n",
    "\n",
    "# TODO: Compile the model with:\n",
    "# - optimizer: Adam with learning_rate=0.001\n",
    "# - loss: 'binary_crossentropy'\n",
    "# - metrics: ['accuracy']\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=___),\n",
    "    loss=___,\n",
    "    metrics=[___]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# TODO: Fill in the model.fit() parameters:\n",
    "# - Use X_train_balanced and y_train_balanced\n",
    "# - validation_split=0.2\n",
    "# - epochs=30\n",
    "# - batch_size=32\n",
    "# - class_weight=class_weight_dict\n",
    "# - callbacks with EarlyStopping monitoring 'val_loss' with patience=5 and restore_best_weights=True\n",
    "# - verbose=1\n",
    "history = model.fit(\n",
    "    ___, ___,\n",
    "    validation_split=___,\n",
    "    epochs=___,\n",
    "    batch_size=___,\n",
    "    class_weight=___,\n",
    "    callbacks=[EarlyStopping(monitor=___, patience=___, restore_best_weights=___)],\n",
    "    verbose=___\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (only run model once)\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred_proba = y_pred_proba.ravel()  # Flatten for easier use\n",
    "y_pred = (y_pred_proba > 0.7017).astype(int)  # Using optimal threshold from ROC analysis\n",
    "\n",
    "# Calculate all metrics efficiently\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix and derived metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "# Calculate percentages (avoid division by zero)\n",
    "cm_percent = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-10) * 100\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:   {test_acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Confusion Matrix with labels and percentages\n",
    "print(\"\\nCONFUSION MATRIX\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'':20s} {'Predicted: No':20s} {'Predicted: Yes':20s}\")\n",
    "print(f\"{'Actual: No':20s} {f'TN={cm[0,0]} ({cm_percent[0,0]:.1f}%)':20s} {f'FP={cm[0,1]} ({cm_percent[0,1]:.1f}%)':20s}\")\n",
    "print(f\"{'Actual: Yes':20s} {f'FN={cm[1,0]} ({cm_percent[1,0]:.1f}%)':20s} {f'TP={cm[1,1]} ({cm_percent[1,1]:.1f}%)':20s}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTotal Test Samples: {len(y_test)}\")\n",
    "print(f\"True Negatives (TN):  {cm[0,0]} ({cm_percent[0,0]:.1f}%)\")\n",
    "print(f\"False Positives (FP): {cm[0,1]} ({cm_percent[0,1]:.1f}%)\")\n",
    "print(f\"False Negatives (FN): {cm[1,0]} ({cm_percent[1,0]:.1f}%)\")\n",
    "print(f\"True Positives (TP):  {cm[1,1]} ({cm_percent[1,1]:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Find optimal threshold using Youden's J statistic\n",
    "youden_j = tpr - fpr\n",
    "optimal_idx = np.argmax(youden_j)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=10, \n",
    "         label=f'Optimal Threshold = {optimal_threshold:.4f}')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ROC analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROC CURVE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to Answer / Explore for the summary section\n",
    "\n",
    "- Evaluate the current performance of the model. What are some things you could try to improve the performance of the model? If you can, implement them and discuss the changes from the current implementation, and whether or not performance improved.\n",
    "\n",
    "- Is model that is at or near 100% feasible?\n",
    "\n",
    "- Which of the classification metrics (Accuracy, Precision, Recall, F1, Specificity) are most important when it comes to identifying customers likely to churn.\n",
    "\n",
    "- change the line of code y_pred = (y_pred_prob > 0.3).astype(int) to either 0.3 or the ROC optimal threshold value. What changes do you see to the confusion matrix? If the Bank were to offer a promotion to the clients identified as a churn risk, which value (0.3 or optimal) would be best to use if the promotion was very cheap? What if it was expensive?\n",
    "\n",
    "- Any other thoughts on this project can be placed here."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
